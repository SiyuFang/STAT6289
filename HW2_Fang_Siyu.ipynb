{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN88I63/wxW5ml7heK9zT3l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SiyuFang/STAT6289/blob/master/HW2_Fang_Siyu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMuSZNwGt6K_"
      },
      "source": [
        "# (a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6nLx_oMOp8y"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrEAZlpfA4JZ"
      },
      "source": [
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "data_augmentation = True\n",
        "num_predictions = 20\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model.h5'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cECaufa-BLq7",
        "outputId": "45fe7ad3-3e6d-49c8-b0ca-06f64426d1b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mG58SMgBRX8"
      },
      "source": [
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "#model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CxxUCgol9mv"
      },
      "source": [
        "#model 1\n",
        "model1 = Sequential()\n",
        "\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(num_classes))\n",
        "model1.add(Activation('softmax'))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXBY6tThmBQN"
      },
      "source": [
        "#model 2\n",
        "model2 = Sequential()\n",
        "model2.add(Flatten())\n",
        "\n",
        "model2.add(Dense(512,input_shape=x_train.shape[1:]))\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Dropout(0.5))\n",
        "\n",
        "model2.add(Dense(num_classes))\n",
        "model2.add(Activation('softmax'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGqiC-wsmD9Q"
      },
      "source": [
        "#Model 3\n",
        "model3 = Sequential()\n",
        "model3.add(Flatten())\n",
        "\n",
        "model3.add(Dense(512,input_shape=x_train.shape[1:]))\n",
        "model3.add(Activation('relu'))\n",
        "model3.add(Dropout(0.5))\n",
        "\n",
        "model3.add(Dense(512))\n",
        "model3.add(Activation('relu'))\n",
        "model3.add(Dropout(0.5))\n",
        "\n",
        "model3.add(Dense(num_classes))\n",
        "model3.add(Activation('softmax'))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYUT6qdDmG-V"
      },
      "source": [
        "#model 4\n",
        "model4 = Sequential()\n",
        "model4.add(Flatten())\n",
        "\n",
        "model4.add(Dense(512,input_shape=x_train.shape[1:]))\n",
        "model4.add(Activation('relu'))\n",
        "model4.add(Dropout(0.5))\n",
        "\n",
        "model4.add(Dense(512))\n",
        "model4.add(Activation('relu'))\n",
        "model4.add(Dropout(0.5))\n",
        "\n",
        "model4.add(Dense(512))\n",
        "model4.add(Activation('relu'))\n",
        "model4.add(Dropout(0.5))\n",
        "\n",
        "model4.add(Dense(num_classes))\n",
        "model4.add(Activation('softmax'))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9HziNRhmJ8J"
      },
      "source": [
        "#model 5\n",
        "model5 = Sequential()\n",
        "model5.add(Flatten())\n",
        "\n",
        "model5.add(Dense(512,input_shape=x_train.shape[1:]))\n",
        "model5.add(Activation('relu'))\n",
        "model5.add(Dropout(0.5))\n",
        "\n",
        "model5.add(Dense(512))\n",
        "model5.add(Activation('relu'))\n",
        "model5.add(Dropout(0.5))\n",
        "\n",
        "model5.add(Dense(512))\n",
        "model5.add(Activation('relu'))\n",
        "model5.add(Dropout(0.5))\n",
        "\n",
        "model5.add(Dense(512))\n",
        "model5.add(Activation('relu'))\n",
        "model5.add(Dropout(0.5))\n",
        "\n",
        "model5.add(Dense(num_classes))\n",
        "model5.add(Activation('softmax'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ulrPdeBBaQl"
      },
      "source": [
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkBPTI9SBfRP",
        "outputId": "dcc188a2-a0eb-47ed-8b3a-0befb4c30390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "model1.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "model3.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "model4.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "model5.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "       # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    history=model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)\n",
        "    history1=model1.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)  \n",
        "    history2=model2.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)  \n",
        "    history3=model3.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)\n",
        "    history4=model4.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)\n",
        "    history5=model5.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "WARNING:tensorflow:From <ipython-input-11-2256813ba096>:75: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8595 - accuracy: 0.3178 - val_loss: 1.7706 - val_accuracy: 0.3708\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.5851 - accuracy: 0.4233 - val_loss: 1.5315 - val_accuracy: 0.4542\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.4691 - accuracy: 0.4683 - val_loss: 1.3459 - val_accuracy: 0.5256\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.3896 - accuracy: 0.4986 - val_loss: 1.2597 - val_accuracy: 0.5474\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.3240 - accuracy: 0.5260 - val_loss: 1.2188 - val_accuracy: 0.5692\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.2702 - accuracy: 0.5469 - val_loss: 1.0983 - val_accuracy: 0.6128\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.2253 - accuracy: 0.5662 - val_loss: 1.0860 - val_accuracy: 0.6187\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.1807 - accuracy: 0.5835 - val_loss: 1.0555 - val_accuracy: 0.6238\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.1478 - accuracy: 0.5955 - val_loss: 1.0433 - val_accuracy: 0.6346\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.1065 - accuracy: 0.6115 - val_loss: 1.0151 - val_accuracy: 0.6436\n",
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 2.0101 - accuracy: 0.2790 - val_loss: 1.8865 - val_accuracy: 0.3401\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.9051 - accuracy: 0.3291 - val_loss: 1.8388 - val_accuracy: 0.3542\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8801 - accuracy: 0.3393 - val_loss: 1.8184 - val_accuracy: 0.3597\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8637 - accuracy: 0.3462 - val_loss: 1.8031 - val_accuracy: 0.3726\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8539 - accuracy: 0.3496 - val_loss: 1.7830 - val_accuracy: 0.3805\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8454 - accuracy: 0.3520 - val_loss: 1.7926 - val_accuracy: 0.3740\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8441 - accuracy: 0.3544 - val_loss: 1.7786 - val_accuracy: 0.3823\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8346 - accuracy: 0.3590 - val_loss: 1.7860 - val_accuracy: 0.3824\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8311 - accuracy: 0.3593 - val_loss: 1.7823 - val_accuracy: 0.3734\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8303 - accuracy: 0.3622 - val_loss: 1.7553 - val_accuracy: 0.3959\n",
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 2.0069 - accuracy: 0.2705 - val_loss: 1.7976 - val_accuracy: 0.3788\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8795 - accuracy: 0.3257 - val_loss: 1.7167 - val_accuracy: 0.4040\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8324 - accuracy: 0.3438 - val_loss: 1.6900 - val_accuracy: 0.4169\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8039 - accuracy: 0.3563 - val_loss: 1.6563 - val_accuracy: 0.4214\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.7830 - accuracy: 0.3628 - val_loss: 1.6226 - val_accuracy: 0.4354\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7697 - accuracy: 0.3719 - val_loss: 1.5997 - val_accuracy: 0.4412\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7539 - accuracy: 0.3743 - val_loss: 1.6018 - val_accuracy: 0.4383\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7381 - accuracy: 0.3843 - val_loss: 1.5678 - val_accuracy: 0.4587\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7273 - accuracy: 0.3886 - val_loss: 1.5687 - val_accuracy: 0.4583\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7155 - accuracy: 0.3909 - val_loss: 1.5403 - val_accuracy: 0.4569\n",
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 2.0940 - accuracy: 0.2269 - val_loss: 1.8650 - val_accuracy: 0.3345\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.9440 - accuracy: 0.2926 - val_loss: 1.7732 - val_accuracy: 0.3780\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8965 - accuracy: 0.3127 - val_loss: 1.7376 - val_accuracy: 0.3978\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8704 - accuracy: 0.3268 - val_loss: 1.6952 - val_accuracy: 0.4033\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8495 - accuracy: 0.3360 - val_loss: 1.6657 - val_accuracy: 0.4145\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8308 - accuracy: 0.3422 - val_loss: 1.6518 - val_accuracy: 0.4259\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8233 - accuracy: 0.3474 - val_loss: 1.6389 - val_accuracy: 0.4291\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8136 - accuracy: 0.3527 - val_loss: 1.6343 - val_accuracy: 0.4290\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8066 - accuracy: 0.3542 - val_loss: 1.6140 - val_accuracy: 0.4424\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 29s 18ms/step - loss: 1.7991 - accuracy: 0.3569 - val_loss: 1.6219 - val_accuracy: 0.4405\n",
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 2.1561 - accuracy: 0.1928 - val_loss: 1.9609 - val_accuracy: 0.3047\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.9979 - accuracy: 0.2643 - val_loss: 1.8306 - val_accuracy: 0.3551\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.9452 - accuracy: 0.2914 - val_loss: 1.7707 - val_accuracy: 0.3715\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.9160 - accuracy: 0.3053 - val_loss: 1.7656 - val_accuracy: 0.3785\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8924 - accuracy: 0.3156 - val_loss: 1.7290 - val_accuracy: 0.3999\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8765 - accuracy: 0.3241 - val_loss: 1.7017 - val_accuracy: 0.4156\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8573 - accuracy: 0.3304 - val_loss: 1.6959 - val_accuracy: 0.4080\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8482 - accuracy: 0.3369 - val_loss: 1.6911 - val_accuracy: 0.4091\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8378 - accuracy: 0.3385 - val_loss: 1.6893 - val_accuracy: 0.4221\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8327 - accuracy: 0.3404 - val_loss: 1.6992 - val_accuracy: 0.4273\n",
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 2.2162 - accuracy: 0.1528 - val_loss: 2.0397 - val_accuracy: 0.2297\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 2.0636 - accuracy: 0.2203 - val_loss: 1.9437 - val_accuracy: 0.2825\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 29s 18ms/step - loss: 2.0034 - accuracy: 0.2520 - val_loss: 1.8766 - val_accuracy: 0.3309\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 29s 18ms/step - loss: 1.9671 - accuracy: 0.2712 - val_loss: 1.8641 - val_accuracy: 0.3447\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 29s 18ms/step - loss: 1.9496 - accuracy: 0.2810 - val_loss: 1.8335 - val_accuracy: 0.3325\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 29s 18ms/step - loss: 1.9368 - accuracy: 0.2898 - val_loss: 1.8599 - val_accuracy: 0.3462\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 29s 18ms/step - loss: 1.9202 - accuracy: 0.2964 - val_loss: 1.8229 - val_accuracy: 0.3747\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.9031 - accuracy: 0.3053 - val_loss: 1.8262 - val_accuracy: 0.3726\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 29s 18ms/step - loss: 1.8967 - accuracy: 0.3100 - val_loss: 1.8741 - val_accuracy: 0.3423\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8868 - accuracy: 0.3131 - val_loss: 1.8304 - val_accuracy: 0.3658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JNXhcqTB6wm",
        "outputId": "d1de4158-58b2-4f89-9c74-62cebdf55f36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved trained model at /content/saved_models/keras_cifar10_trained_model.h5 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaY8xHybB9Nw",
        "outputId": "120ff457-8fd4-45d5-9bfa-a57345216ae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 1.0151 - accuracy: 0.6436\n",
            "Test loss: 1.015108346939087\n",
            "Test accuracy: 0.6435999870300293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPDjjcYXn0pu",
        "outputId": "e9c18319-dc9f-4d5a-bd0c-bd880ffe4bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "print(history1.history.keys())\n",
        "print(history2.history.keys())\n",
        "print(history3.history.keys())\n",
        "print(history4.history.keys())\n",
        "print(history5.history.keys())\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.plot(history1.history['val_accuracy'])\n",
        "plt.plot(history2.history['val_accuracy'])\n",
        "plt.plot(history3.history['val_accuracy'])\n",
        "plt.plot(history4.history['val_accuracy'])\n",
        "plt.plot(history5.history['val_accuracy'])\n",
        "plt.title('Test Accuracy of Various Models on CIFAR-10')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['CNN','0 hidden layer','1 hidden layer','2 hidden layer','3 hidden layer','4 hidden layer','5 hidden layer'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k10k2Ga7uIc2"
      },
      "source": [
        "According to the plot, the convolutional network has the best test accuracy result and the simple dense neural networks with 4 hidden layers has the worse test accuracy result. From the graph, we can see as we increse the number of dense hidden layers, the optimation is worse and worse, except the 0 dense hidden layers. It's because the convolutional network distributes shared weights, and it simplifies the optimation.But when we increase the number of dense hidden layers, the optimation becomes more difficulte and fails to find the best weight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oVQKMfmtxYb"
      },
      "source": [
        "# (b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "madifWW4tvXq",
        "outputId": "e499875a-8396-4da3-c5cd-839fce94b867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "data_augmentation = False\n",
        "data1_augmentation = True\n",
        "num_predictions = 20\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model.h5'\n",
        "\n",
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "#Model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "#model 1\n",
        "model1 = Sequential()\n",
        "model1.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model1.add(Activation('sigmoid'))\n",
        "model1.add(Conv2D(32, (3, 3)))\n",
        "model1.add(Activation('sigmoid'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model1.add(Dropout(0.25))\n",
        "\n",
        "model1.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model1.add(Activation('sigmoid'))\n",
        "model1.add(Conv2D(64, (3, 3)))\n",
        "model1.add(Activation('sigmoid'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model1.add(Dropout(0.25))\n",
        "\n",
        "model1.add(Flatten())\n",
        "model1.add(Activation('sigmoid'))\n",
        "model1.add(Dropout(0.5))\n",
        "model1.add(Dense(num_classes))\n",
        "model1.add(Activation('softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "model1.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        shuffle=True)\n",
        "    history1 = model1.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,  # set each sample mean to 0\n",
        "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,  # apply ZCA whitening\n",
        "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            # randomly shift images horizontally (fraction of total width)\n",
        "            width_shift_range=0.1,\n",
        "            # randomly shift images vertically (fraction of total height)\n",
        "            height_shift_range=0.1,\n",
        "            shear_range=0.,  # set range for random shear\n",
        "            zoom_range=0.,  # set range for random zoom\n",
        "            channel_shift_range=0.,  # set range for random channel shifts\n",
        "            # set mode for filling points outside the input boundaries\n",
        "            fill_mode='nearest',\n",
        "            cval=0.,  # value used for fill_mode = \"constant\"\n",
        "            horizontal_flip=True,  # randomly flip images\n",
        "            vertical_flip=False,  # randomly flip images\n",
        "            # set rescaling factor (applied before any other transformation)\n",
        "            rescale=None,\n",
        "            # set function that will be applied on each input\n",
        "            preprocessing_function=None,\n",
        "            # image data format, either \"channels_first\" or \"channels_last\"\n",
        "            data_format=None,\n",
        "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "            validation_split=0.0)\n",
        "    \n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    history = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)\n",
        "    history1 = model1.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)\n",
        "    \n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "print(history1.history.keys())\n",
        "\n",
        "# summarize history for test accuracy\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.plot(history1.history['val_accuracy'])\n",
        "plt.title('Test Accuracy of CNN on CIFAR-10 using ReLu vs Sigmoid Activation')\n",
        "plt.ylabel('test accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['ReLU','sigmoid'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "Not using data augmentation.\n",
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8273 - accuracy: 0.3329 - val_loss: 1.6655 - val_accuracy: 0.3948\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5340 - accuracy: 0.4473 - val_loss: 1.3709 - val_accuracy: 0.5078\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3935 - accuracy: 0.4998 - val_loss: 1.2979 - val_accuracy: 0.5393\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2991 - accuracy: 0.5371 - val_loss: 1.1716 - val_accuracy: 0.5819\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2241 - accuracy: 0.5674 - val_loss: 1.1273 - val_accuracy: 0.6051\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1595 - accuracy: 0.5891 - val_loss: 1.0645 - val_accuracy: 0.6219\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1049 - accuracy: 0.6120 - val_loss: 1.0117 - val_accuracy: 0.6472\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0529 - accuracy: 0.6308 - val_loss: 0.9986 - val_accuracy: 0.6538\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0078 - accuracy: 0.6491 - val_loss: 0.9303 - val_accuracy: 0.6777\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9707 - accuracy: 0.6613 - val_loss: 0.9274 - val_accuracy: 0.6814\n",
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.5241 - accuracy: 0.1010 - val_loss: 2.3134 - val_accuracy: 0.1000\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.4861 - accuracy: 0.1022 - val_loss: 2.3160 - val_accuracy: 0.1000\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.4581 - accuracy: 0.1000 - val_loss: 2.3178 - val_accuracy: 0.1000\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.4379 - accuracy: 0.0999 - val_loss: 2.3222 - val_accuracy: 0.1000\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.4135 - accuracy: 0.0998 - val_loss: 2.3177 - val_accuracy: 0.1000\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3991 - accuracy: 0.1006 - val_loss: 2.3310 - val_accuracy: 0.1000\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3864 - accuracy: 0.0998 - val_loss: 2.3169 - val_accuracy: 0.1000\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3815 - accuracy: 0.0976 - val_loss: 2.3118 - val_accuracy: 0.1000\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3669 - accuracy: 0.1021 - val_loss: 2.3366 - val_accuracy: 0.1000\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3623 - accuracy: 0.0983 - val_loss: 2.3123 - val_accuracy: 0.1000\n",
            "Saved trained model at /content/saved_models/keras_cifar10_trained_model.h5 \n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9274 - accuracy: 0.6814\n",
            "Test loss: 0.927416205406189\n",
            "Test accuracy: 0.6814000010490417\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lycfOYljvbnK"
      },
      "source": [
        "The rectified linear units has much better accuracy results than the sigmoid units. The lack of improvement in the first few epochs of the sigmoid units might be due to the fact that the starting point is in one of the tails of the sigmoid whose gradient is near 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LXwzzttvcRL"
      },
      "source": [
        "# (c)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIqbXJMSvdGp",
        "outputId": "76af0f4f-fc1c-4856-8fd2-445af2208e95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "data_augmentation = False\n",
        "data1_augmentation = True\n",
        "num_predictions = 20\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model.h5'\n",
        "\n",
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "#Model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "#model 1\n",
        "model1 = Sequential()\n",
        "model1.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(Conv2D(32, (3, 3)))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model1.add(Dropout(0.25))\n",
        "\n",
        "model1.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(Conv2D(64, (3, 3)))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model1.add(Dropout(0.25))\n",
        "\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(512))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(Dropout(0.5))\n",
        "model1.add(Dense(num_classes))\n",
        "model1.add(Activation('softmax'))\n",
        "\n",
        "#model 2\n",
        "model2 = Sequential()\n",
        "model2.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Conv2D(32, (3, 3)))\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model2.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Conv2D(64, (3, 3)))\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(512))\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Dense(num_classes))\n",
        "model2.add(Activation('softmax'))\n",
        "\n",
        "#model 3\n",
        "model3 = Sequential()\n",
        "model3.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model3.add(Activation('relu'))\n",
        "model3.add(Conv2D(32, (3, 3)))\n",
        "model3.add(Activation('relu'))\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model3.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model3.add(Activation('relu'))\n",
        "model3.add(Conv2D(64, (3, 3)))\n",
        "model3.add(Activation('relu'))\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(512))\n",
        "model3.add(Activation('relu'))\n",
        "model3.add(Dense(num_classes))\n",
        "model3.add(Activation('softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "model1.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "model2.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "model3.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        shuffle=True)\n",
        "    history2 = model2.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        shuffle=True)\n",
        "else:\n",
        "  print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "  datagen = ImageDataGenerator(\n",
        "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,  # set each sample mean to 0\n",
        "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,  # apply ZCA whitening\n",
        "            zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "            rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            # randomly shift images horizontally (fraction of total width)\n",
        "            width_shift_range=0.1,\n",
        "            # randomly shift images vertically (fraction of total height)\n",
        "            height_shift_range=0.1,\n",
        "            shear_range=0.,  # set range for random shear\n",
        "            zoom_range=0.,  # set range for random zoom\n",
        "            channel_shift_range=0.,  # set range for random channel shifts\n",
        "            # set mode for filling points outside the input boundaries\n",
        "            fill_mode='nearest',\n",
        "            cval=0.,  # value used for fill_mode = \"constant\"\n",
        "            horizontal_flip=True,  # randomly flip images\n",
        "            vertical_flip=False,  # randomly flip images\n",
        "            # set rescaling factor (applied before any other transformation)\n",
        "            rescale=None,\n",
        "            # set function that will be applied on each input\n",
        "            preprocessing_function=None,\n",
        "            # image data format, either \"channels_first\" or \"channels_last\"\n",
        "            data_format=None,\n",
        "            # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "            validation_split=0.0)\n",
        "    \n",
        "  # Compute quantities required for feature-wise normalization\n",
        "  # (std, mean, and principal components if ZCA whitening is applied).\n",
        "  datagen.fit(x_train)\n",
        "\n",
        "  # Fit the model on the batches generated by datagen.flow().\n",
        "  history = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)\n",
        "  history2 = model2.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)\n",
        "    \n",
        "if not data1_augmentation:\n",
        "   history1 = model1.fit(x_train, y_train,\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=epochs,\n",
        "                          alidation_data=(x_test, y_test),\n",
        "                          shuffle=True)\n",
        "   history3 = model3.fit(x_train, y_train,\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=epochs,\n",
        "                          alidation_data=(x_test, y_test),\n",
        "                          shuffle=True)\n",
        "    \n",
        "else:\n",
        "  print('Using real-time data augmentation.')\n",
        "  # This will do preprocessing and realtime data augmentation:\n",
        "  datagen = ImageDataGenerator(\n",
        "          featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "          samplewise_center=False,  # set each sample mean to 0\n",
        "          featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "          samplewise_std_normalization=False,  # divide each input by its std\n",
        "          zca_whitening=False,  # apply ZCA whitening\n",
        "          zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "          rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "          # randomly shift images horizontally (fraction of total width)\n",
        "          width_shift_range=0.1,\n",
        "          # randomly shift images vertically (fraction of total height)\n",
        "          height_shift_range=0.1,\n",
        "          shear_range=0.,  # set range for random shear\n",
        "          zoom_range=0.,  # set range for random zoom\n",
        "          channel_shift_range=0.,  # set range for random channel shifts\n",
        "          # set mode for filling points outside the input boundaries\n",
        "          fill_mode='nearest',\n",
        "          cval=0.,  # value used for fill_mode = \"constant\"\n",
        "          horizontal_flip=True,  # randomly flip images\n",
        "          vertical_flip=False,  # randomly flip images\n",
        "          # set rescaling factor (applied before any other transformation)\n",
        "          rescale=None,\n",
        "          # set function that will be applied on each input\n",
        "          preprocessing_function=None,\n",
        "          # image data format, either \"channels_first\" or \"channels_last\"\n",
        "          data_format=None,\n",
        "          # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "          validation_split=0.0)\n",
        "    \n",
        "  # Compute quantities required for feature-wise normalization\n",
        "  # (std, mean, and principal components if ZCA whitening is applied).\n",
        "  datagen.fit(x_train)\n",
        "\n",
        "  # Fit the model on the batches generated by datagen.flow().\n",
        "  history1 = model1.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                      epochs=epochs,\n",
        "                      validation_data=(x_test, y_test),\n",
        "                      workers=4)\n",
        "  history3 = model3.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                      epochs=epochs,\n",
        "                      validation_data=(x_test, y_test),\n",
        "                      workers=4)\n",
        "    \n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "print(history1.history.keys())\n",
        "print(history2.history.keys())\n",
        "print(history3.history.keys())\n",
        "\n",
        "# summarize history for training accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history2.history['accuracy'])\n",
        "plt.plot(history1.history['accuracy'])\n",
        "plt.plot(history3.history['accuracy'])\n",
        "plt.title('training accuracy')\n",
        "plt.ylabel('training accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['with dropout without augument','without dropout without augument','with dropout with augument','without dropout with augument'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for test accuracy\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.plot(history2.history['val_accuracy'])\n",
        "plt.plot(history1.history['val_accuracy'])\n",
        "plt.plot(history3.history['val_accuracy'])\n",
        "plt.title('validation accuracy')\n",
        "plt.ylabel('validation accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['with dropout without augument','without dropout without augument','with dropout with augument','without dropout with augument'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "Not using data augmentation.\n",
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8022 - accuracy: 0.3413 - val_loss: 1.5139 - val_accuracy: 0.4468\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4867 - accuracy: 0.4603 - val_loss: 1.3703 - val_accuracy: 0.5149\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3517 - accuracy: 0.5133 - val_loss: 1.2303 - val_accuracy: 0.5675\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2585 - accuracy: 0.5525 - val_loss: 1.1581 - val_accuracy: 0.5916\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1793 - accuracy: 0.5838 - val_loss: 1.0953 - val_accuracy: 0.6127\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1127 - accuracy: 0.6090 - val_loss: 1.0288 - val_accuracy: 0.6443\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0541 - accuracy: 0.6287 - val_loss: 1.0190 - val_accuracy: 0.6394\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0097 - accuracy: 0.6462 - val_loss: 0.9357 - val_accuracy: 0.6779\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9716 - accuracy: 0.6605 - val_loss: 0.9472 - val_accuracy: 0.6739\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9349 - accuracy: 0.6740 - val_loss: 0.8812 - val_accuracy: 0.6940\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9082 - accuracy: 0.6839 - val_loss: 0.8687 - val_accuracy: 0.6958\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8836 - accuracy: 0.6916 - val_loss: 0.9223 - val_accuracy: 0.6697\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8643 - accuracy: 0.6984 - val_loss: 0.8102 - val_accuracy: 0.7185\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8412 - accuracy: 0.7063 - val_loss: 0.8842 - val_accuracy: 0.6960\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8283 - accuracy: 0.7109 - val_loss: 0.8060 - val_accuracy: 0.7251\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8104 - accuracy: 0.7180 - val_loss: 0.7753 - val_accuracy: 0.7339\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8000 - accuracy: 0.7227 - val_loss: 0.7893 - val_accuracy: 0.7294\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7839 - accuracy: 0.7298 - val_loss: 0.7613 - val_accuracy: 0.7383\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7773 - accuracy: 0.7320 - val_loss: 0.7550 - val_accuracy: 0.7386\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7676 - accuracy: 0.7331 - val_loss: 0.7799 - val_accuracy: 0.7369\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7553 - accuracy: 0.7385 - val_loss: 0.7414 - val_accuracy: 0.7462\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7466 - accuracy: 0.7442 - val_loss: 0.7273 - val_accuracy: 0.7545\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7439 - accuracy: 0.7436 - val_loss: 0.7481 - val_accuracy: 0.7431\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7304 - accuracy: 0.7499 - val_loss: 0.7547 - val_accuracy: 0.7449\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7288 - accuracy: 0.7527 - val_loss: 0.7331 - val_accuracy: 0.7498\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7234 - accuracy: 0.7560 - val_loss: 0.7131 - val_accuracy: 0.7577\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7174 - accuracy: 0.7545 - val_loss: 0.7071 - val_accuracy: 0.7604\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7110 - accuracy: 0.7590 - val_loss: 0.6977 - val_accuracy: 0.7609\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7123 - accuracy: 0.7590 - val_loss: 0.7203 - val_accuracy: 0.7534\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7042 - accuracy: 0.7618 - val_loss: 0.7111 - val_accuracy: 0.7659\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6977 - accuracy: 0.7618 - val_loss: 0.6958 - val_accuracy: 0.7635\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6961 - accuracy: 0.7625 - val_loss: 0.7192 - val_accuracy: 0.7580\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6879 - accuracy: 0.7673 - val_loss: 0.7027 - val_accuracy: 0.7613\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6896 - accuracy: 0.7662 - val_loss: 0.6916 - val_accuracy: 0.7665\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6811 - accuracy: 0.7701 - val_loss: 0.7190 - val_accuracy: 0.7670\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6859 - accuracy: 0.7702 - val_loss: 0.6763 - val_accuracy: 0.7716\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6758 - accuracy: 0.7723 - val_loss: 0.6814 - val_accuracy: 0.7746\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6805 - accuracy: 0.7722 - val_loss: 0.7374 - val_accuracy: 0.7671\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6761 - accuracy: 0.7720 - val_loss: 0.6696 - val_accuracy: 0.7752\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6734 - accuracy: 0.7746 - val_loss: 0.7145 - val_accuracy: 0.7720\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6697 - accuracy: 0.7742 - val_loss: 0.6558 - val_accuracy: 0.7727\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6676 - accuracy: 0.7746 - val_loss: 0.6802 - val_accuracy: 0.7728\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6668 - accuracy: 0.7754 - val_loss: 0.6866 - val_accuracy: 0.7693\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6681 - accuracy: 0.7776 - val_loss: 0.6737 - val_accuracy: 0.7778\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6603 - accuracy: 0.7787 - val_loss: 0.6570 - val_accuracy: 0.7797\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6592 - accuracy: 0.7812 - val_loss: 0.6601 - val_accuracy: 0.7812\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6552 - accuracy: 0.7786 - val_loss: 0.6905 - val_accuracy: 0.7716\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6532 - accuracy: 0.7795 - val_loss: 0.6785 - val_accuracy: 0.7783\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6519 - accuracy: 0.7825 - val_loss: 0.6351 - val_accuracy: 0.7844\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6554 - accuracy: 0.7796 - val_loss: 0.6547 - val_accuracy: 0.7797\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6528 - accuracy: 0.7815 - val_loss: 0.6851 - val_accuracy: 0.7801\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6545 - accuracy: 0.7810 - val_loss: 0.7205 - val_accuracy: 0.7670\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6520 - accuracy: 0.7825 - val_loss: 0.7074 - val_accuracy: 0.7708\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6492 - accuracy: 0.7832 - val_loss: 0.7105 - val_accuracy: 0.7646\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6421 - accuracy: 0.7874 - val_loss: 0.6997 - val_accuracy: 0.7756\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6412 - accuracy: 0.7857 - val_loss: 0.6902 - val_accuracy: 0.7675\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6431 - accuracy: 0.7850 - val_loss: 0.6598 - val_accuracy: 0.7807\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6417 - accuracy: 0.7886 - val_loss: 0.7087 - val_accuracy: 0.7787\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6462 - accuracy: 0.7829 - val_loss: 0.6913 - val_accuracy: 0.7832\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6388 - accuracy: 0.7874 - val_loss: 0.6695 - val_accuracy: 0.7778\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6426 - accuracy: 0.7853 - val_loss: 0.6794 - val_accuracy: 0.7814\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6398 - accuracy: 0.7878 - val_loss: 0.7105 - val_accuracy: 0.7592\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6367 - accuracy: 0.7880 - val_loss: 0.6665 - val_accuracy: 0.7759\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6381 - accuracy: 0.7850 - val_loss: 0.6752 - val_accuracy: 0.7811\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6348 - accuracy: 0.7886 - val_loss: 0.6996 - val_accuracy: 0.7643\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6375 - accuracy: 0.7853 - val_loss: 0.6549 - val_accuracy: 0.7832\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6359 - accuracy: 0.7870 - val_loss: 0.6987 - val_accuracy: 0.7736\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6395 - accuracy: 0.7887 - val_loss: 0.7231 - val_accuracy: 0.7620\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6378 - accuracy: 0.7877 - val_loss: 0.7230 - val_accuracy: 0.7747\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6385 - accuracy: 0.7877 - val_loss: 0.7396 - val_accuracy: 0.7646\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6369 - accuracy: 0.7884 - val_loss: 0.7216 - val_accuracy: 0.7581\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6358 - accuracy: 0.7876 - val_loss: 0.7247 - val_accuracy: 0.7605\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6352 - accuracy: 0.7886 - val_loss: 0.7250 - val_accuracy: 0.7828\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6363 - accuracy: 0.7882 - val_loss: 0.6663 - val_accuracy: 0.7752\n",
            "Epoch 75/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6357 - accuracy: 0.7882 - val_loss: 0.6735 - val_accuracy: 0.7759\n",
            "Epoch 76/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6398 - accuracy: 0.7880 - val_loss: 0.6918 - val_accuracy: 0.7683\n",
            "Epoch 77/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6341 - accuracy: 0.7888 - val_loss: 0.6657 - val_accuracy: 0.7823\n",
            "Epoch 78/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6380 - accuracy: 0.7869 - val_loss: 0.6592 - val_accuracy: 0.7808\n",
            "Epoch 79/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6346 - accuracy: 0.7900 - val_loss: 0.7141 - val_accuracy: 0.7668\n",
            "Epoch 80/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6374 - accuracy: 0.7881 - val_loss: 0.6952 - val_accuracy: 0.7748\n",
            "Epoch 81/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6416 - accuracy: 0.7869 - val_loss: 0.6804 - val_accuracy: 0.7789\n",
            "Epoch 82/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6363 - accuracy: 0.7898 - val_loss: 0.6548 - val_accuracy: 0.7851\n",
            "Epoch 83/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6338 - accuracy: 0.7891 - val_loss: 0.6579 - val_accuracy: 0.7840\n",
            "Epoch 84/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6378 - accuracy: 0.7867 - val_loss: 0.6671 - val_accuracy: 0.7785\n",
            "Epoch 85/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6359 - accuracy: 0.7874 - val_loss: 0.7172 - val_accuracy: 0.7772\n",
            "Epoch 86/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6382 - accuracy: 0.7886 - val_loss: 0.6801 - val_accuracy: 0.7754\n",
            "Epoch 87/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6337 - accuracy: 0.7896 - val_loss: 0.6670 - val_accuracy: 0.7851\n",
            "Epoch 88/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6360 - accuracy: 0.7896 - val_loss: 0.6652 - val_accuracy: 0.7796\n",
            "Epoch 89/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6350 - accuracy: 0.7889 - val_loss: 0.6694 - val_accuracy: 0.7810\n",
            "Epoch 90/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6394 - accuracy: 0.7887 - val_loss: 0.6658 - val_accuracy: 0.7805\n",
            "Epoch 91/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6353 - accuracy: 0.7886 - val_loss: 0.7034 - val_accuracy: 0.7738\n",
            "Epoch 92/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6384 - accuracy: 0.7892 - val_loss: 0.6827 - val_accuracy: 0.7729\n",
            "Epoch 93/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6435 - accuracy: 0.7873 - val_loss: 0.6640 - val_accuracy: 0.7827\n",
            "Epoch 94/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6404 - accuracy: 0.7871 - val_loss: 0.7054 - val_accuracy: 0.7666\n",
            "Epoch 95/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6411 - accuracy: 0.7877 - val_loss: 0.7195 - val_accuracy: 0.7697\n",
            "Epoch 96/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6392 - accuracy: 0.7883 - val_loss: 0.7737 - val_accuracy: 0.7645\n",
            "Epoch 97/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6429 - accuracy: 0.7886 - val_loss: 0.8748 - val_accuracy: 0.7192\n",
            "Epoch 98/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6437 - accuracy: 0.7888 - val_loss: 0.7384 - val_accuracy: 0.7573\n",
            "Epoch 99/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6421 - accuracy: 0.7891 - val_loss: 0.7037 - val_accuracy: 0.7739\n",
            "Epoch 100/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6398 - accuracy: 0.7903 - val_loss: 0.7067 - val_accuracy: 0.7711\n",
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6784 - accuracy: 0.3977 - val_loss: 1.5188 - val_accuracy: 0.4632\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3786 - accuracy: 0.5100 - val_loss: 1.2951 - val_accuracy: 0.5370\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2462 - accuracy: 0.5618 - val_loss: 1.2529 - val_accuracy: 0.5566\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.1402 - accuracy: 0.6006 - val_loss: 1.1390 - val_accuracy: 0.5967\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.0553 - accuracy: 0.6307 - val_loss: 1.0653 - val_accuracy: 0.6282\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.9804 - accuracy: 0.6575 - val_loss: 1.0271 - val_accuracy: 0.6436\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.9163 - accuracy: 0.6826 - val_loss: 1.0040 - val_accuracy: 0.6475\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8594 - accuracy: 0.7049 - val_loss: 0.9375 - val_accuracy: 0.6783\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8090 - accuracy: 0.7224 - val_loss: 1.0129 - val_accuracy: 0.6505\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7574 - accuracy: 0.7391 - val_loss: 0.9225 - val_accuracy: 0.6806\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.7116 - accuracy: 0.7568 - val_loss: 0.8808 - val_accuracy: 0.6991\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6637 - accuracy: 0.7707 - val_loss: 0.8533 - val_accuracy: 0.7097\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6184 - accuracy: 0.7880 - val_loss: 0.9036 - val_accuracy: 0.6983\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.5746 - accuracy: 0.8039 - val_loss: 0.9231 - val_accuracy: 0.6976\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.5323 - accuracy: 0.8191 - val_loss: 0.9316 - val_accuracy: 0.6994\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4898 - accuracy: 0.8349 - val_loss: 0.9052 - val_accuracy: 0.7034\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4491 - accuracy: 0.8479 - val_loss: 0.8928 - val_accuracy: 0.7176\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4084 - accuracy: 0.8638 - val_loss: 0.9106 - val_accuracy: 0.7218\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3701 - accuracy: 0.8753 - val_loss: 0.9259 - val_accuracy: 0.7248\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3340 - accuracy: 0.8891 - val_loss: 0.9466 - val_accuracy: 0.7207\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.2986 - accuracy: 0.9001 - val_loss: 0.9964 - val_accuracy: 0.7275\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.2633 - accuracy: 0.9131 - val_loss: 1.0054 - val_accuracy: 0.7301\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.2317 - accuracy: 0.9243 - val_loss: 1.1062 - val_accuracy: 0.7118\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.2041 - accuracy: 0.9340 - val_loss: 1.1347 - val_accuracy: 0.7194\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1741 - accuracy: 0.9446 - val_loss: 1.2226 - val_accuracy: 0.7084\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.1516 - accuracy: 0.9523 - val_loss: 1.2568 - val_accuracy: 0.7155\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.1301 - accuracy: 0.9585 - val_loss: 1.3342 - val_accuracy: 0.7168\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.1099 - accuracy: 0.9661 - val_loss: 1.3820 - val_accuracy: 0.7217\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0939 - accuracy: 0.9706 - val_loss: 1.4125 - val_accuracy: 0.7277\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0789 - accuracy: 0.9747 - val_loss: 1.5542 - val_accuracy: 0.7147\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0688 - accuracy: 0.9790 - val_loss: 1.6088 - val_accuracy: 0.7159\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0585 - accuracy: 0.9831 - val_loss: 1.7313 - val_accuracy: 0.7152\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0518 - accuracy: 0.9840 - val_loss: 1.8081 - val_accuracy: 0.7151\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0445 - accuracy: 0.9863 - val_loss: 1.8480 - val_accuracy: 0.7165\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0393 - accuracy: 0.9879 - val_loss: 1.9816 - val_accuracy: 0.7100\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0355 - accuracy: 0.9891 - val_loss: 1.9969 - val_accuracy: 0.7193\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0319 - accuracy: 0.9901 - val_loss: 1.9820 - val_accuracy: 0.7247\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0283 - accuracy: 0.9918 - val_loss: 2.0726 - val_accuracy: 0.7181\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0262 - accuracy: 0.9918 - val_loss: 2.1655 - val_accuracy: 0.7189\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0250 - accuracy: 0.9916 - val_loss: 2.1610 - val_accuracy: 0.7254\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0218 - accuracy: 0.9928 - val_loss: 2.5186 - val_accuracy: 0.6947\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0218 - accuracy: 0.9934 - val_loss: 2.3432 - val_accuracy: 0.7213\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0197 - accuracy: 0.9935 - val_loss: 2.4067 - val_accuracy: 0.7255\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0201 - accuracy: 0.9938 - val_loss: 2.4206 - val_accuracy: 0.7191\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0189 - accuracy: 0.9940 - val_loss: 2.4761 - val_accuracy: 0.7187\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0183 - accuracy: 0.9942 - val_loss: 2.4598 - val_accuracy: 0.7242\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0168 - accuracy: 0.9948 - val_loss: 2.5317 - val_accuracy: 0.7274\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0170 - accuracy: 0.9945 - val_loss: 2.7003 - val_accuracy: 0.7171\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0158 - accuracy: 0.9952 - val_loss: 2.5932 - val_accuracy: 0.7248\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0139 - accuracy: 0.9956 - val_loss: 2.6043 - val_accuracy: 0.7198\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0137 - accuracy: 0.9958 - val_loss: 2.6901 - val_accuracy: 0.7225\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0134 - accuracy: 0.9955 - val_loss: 2.7541 - val_accuracy: 0.7219\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0138 - accuracy: 0.9955 - val_loss: 2.8738 - val_accuracy: 0.7107\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0138 - accuracy: 0.9955 - val_loss: 2.8472 - val_accuracy: 0.7193\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0133 - accuracy: 0.9960 - val_loss: 3.0212 - val_accuracy: 0.7118\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0134 - accuracy: 0.9958 - val_loss: 3.2015 - val_accuracy: 0.7022\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0136 - accuracy: 0.9960 - val_loss: 3.0350 - val_accuracy: 0.7132\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0132 - accuracy: 0.9959 - val_loss: 3.0554 - val_accuracy: 0.7226\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0134 - accuracy: 0.9956 - val_loss: 2.9742 - val_accuracy: 0.7219\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0122 - accuracy: 0.9957 - val_loss: 3.1126 - val_accuracy: 0.7146\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0123 - accuracy: 0.9963 - val_loss: 3.0574 - val_accuracy: 0.7218\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0124 - accuracy: 0.9957 - val_loss: 3.3144 - val_accuracy: 0.7051\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0116 - accuracy: 0.9963 - val_loss: 3.1489 - val_accuracy: 0.7217\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0115 - accuracy: 0.9965 - val_loss: 3.1496 - val_accuracy: 0.7277\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0107 - accuracy: 0.9963 - val_loss: 3.1654 - val_accuracy: 0.7224\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0124 - accuracy: 0.9962 - val_loss: 3.3528 - val_accuracy: 0.7233\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0117 - accuracy: 0.9962 - val_loss: 3.1775 - val_accuracy: 0.7227\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0112 - accuracy: 0.9962 - val_loss: 3.4264 - val_accuracy: 0.7140\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0101 - accuracy: 0.9966 - val_loss: 3.3358 - val_accuracy: 0.7209\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 3.2782 - val_accuracy: 0.7212\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0105 - accuracy: 0.9966 - val_loss: 3.3392 - val_accuracy: 0.7214\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0103 - accuracy: 0.9968 - val_loss: 3.5283 - val_accuracy: 0.7129\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0104 - accuracy: 0.9963 - val_loss: 3.3254 - val_accuracy: 0.7277\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0101 - accuracy: 0.9967 - val_loss: 3.4348 - val_accuracy: 0.7244\n",
            "Epoch 75/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0110 - accuracy: 0.9965 - val_loss: 3.4200 - val_accuracy: 0.7237\n",
            "Epoch 76/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 3.4429 - val_accuracy: 0.7247\n",
            "Epoch 77/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0100 - accuracy: 0.9968 - val_loss: 3.6219 - val_accuracy: 0.7260\n",
            "Epoch 78/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 3.5759 - val_accuracy: 0.7261\n",
            "Epoch 79/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0104 - accuracy: 0.9965 - val_loss: 3.5729 - val_accuracy: 0.7256\n",
            "Epoch 80/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 3.7473 - val_accuracy: 0.7223\n",
            "Epoch 81/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0103 - accuracy: 0.9970 - val_loss: 3.5009 - val_accuracy: 0.7238\n",
            "Epoch 82/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 3.8436 - val_accuracy: 0.7210\n",
            "Epoch 83/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0086 - accuracy: 0.9971 - val_loss: 3.6747 - val_accuracy: 0.7302\n",
            "Epoch 84/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0088 - accuracy: 0.9970 - val_loss: 3.8408 - val_accuracy: 0.7213\n",
            "Epoch 85/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0102 - accuracy: 0.9971 - val_loss: 3.8572 - val_accuracy: 0.7146\n",
            "Epoch 86/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0090 - accuracy: 0.9972 - val_loss: 3.7686 - val_accuracy: 0.7261\n",
            "Epoch 87/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0087 - accuracy: 0.9967 - val_loss: 3.7428 - val_accuracy: 0.7129\n",
            "Epoch 88/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0080 - accuracy: 0.9972 - val_loss: 3.8412 - val_accuracy: 0.7157\n",
            "Epoch 89/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 3.8346 - val_accuracy: 0.7209\n",
            "Epoch 90/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0097 - accuracy: 0.9968 - val_loss: 3.9453 - val_accuracy: 0.7186\n",
            "Epoch 91/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0101 - accuracy: 0.9968 - val_loss: 3.9113 - val_accuracy: 0.7235\n",
            "Epoch 92/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 3.9964 - val_accuracy: 0.7144\n",
            "Epoch 93/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0090 - accuracy: 0.9970 - val_loss: 4.1219 - val_accuracy: 0.7212\n",
            "Epoch 94/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0097 - accuracy: 0.9970 - val_loss: 3.9755 - val_accuracy: 0.7224\n",
            "Epoch 95/100\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0087 - accuracy: 0.9971 - val_loss: 3.9588 - val_accuracy: 0.7240\n",
            "Epoch 96/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0096 - accuracy: 0.9967 - val_loss: 3.9660 - val_accuracy: 0.7253\n",
            "Epoch 97/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0081 - accuracy: 0.9974 - val_loss: 3.9427 - val_accuracy: 0.7242\n",
            "Epoch 98/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0083 - accuracy: 0.9974 - val_loss: 4.0265 - val_accuracy: 0.7165\n",
            "Epoch 99/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 3.9844 - val_accuracy: 0.7237\n",
            "Epoch 100/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0107 - accuracy: 0.9970 - val_loss: 4.1527 - val_accuracy: 0.7176\n",
            "Using real-time data augmentation.\n",
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.9572 - accuracy: 0.2832 - val_loss: 1.6712 - val_accuracy: 0.4031\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.6574 - accuracy: 0.3953 - val_loss: 1.4910 - val_accuracy: 0.4553\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.5347 - accuracy: 0.4434 - val_loss: 1.4070 - val_accuracy: 0.4875\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.4529 - accuracy: 0.4741 - val_loss: 1.3224 - val_accuracy: 0.5284\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.3955 - accuracy: 0.4994 - val_loss: 1.2799 - val_accuracy: 0.5380\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.3323 - accuracy: 0.5212 - val_loss: 1.1641 - val_accuracy: 0.5823\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.2845 - accuracy: 0.5413 - val_loss: 1.1872 - val_accuracy: 0.5815\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.2359 - accuracy: 0.5583 - val_loss: 1.1722 - val_accuracy: 0.5803\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.1981 - accuracy: 0.5767 - val_loss: 1.0952 - val_accuracy: 0.6121\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.1639 - accuracy: 0.5854 - val_loss: 1.1008 - val_accuracy: 0.6116\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.1383 - accuracy: 0.5989 - val_loss: 0.9752 - val_accuracy: 0.6533\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.1092 - accuracy: 0.6075 - val_loss: 1.0277 - val_accuracy: 0.6381\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.0818 - accuracy: 0.6185 - val_loss: 1.0168 - val_accuracy: 0.6451\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.0568 - accuracy: 0.6292 - val_loss: 0.9395 - val_accuracy: 0.6664\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 1.0390 - accuracy: 0.6344 - val_loss: 0.8972 - val_accuracy: 0.6815\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.0222 - accuracy: 0.6421 - val_loss: 0.9229 - val_accuracy: 0.6761\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 1.0131 - accuracy: 0.6435 - val_loss: 0.8757 - val_accuracy: 0.6952\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.9930 - accuracy: 0.6498 - val_loss: 0.9884 - val_accuracy: 0.6673\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.9787 - accuracy: 0.6563 - val_loss: 0.9043 - val_accuracy: 0.6827\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.9678 - accuracy: 0.6597 - val_loss: 0.8793 - val_accuracy: 0.6959\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.9563 - accuracy: 0.6660 - val_loss: 0.8529 - val_accuracy: 0.7011\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.9430 - accuracy: 0.6683 - val_loss: 0.8521 - val_accuracy: 0.7057\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.9317 - accuracy: 0.6774 - val_loss: 0.8605 - val_accuracy: 0.7007\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.9304 - accuracy: 0.6761 - val_loss: 0.8115 - val_accuracy: 0.7210\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.9142 - accuracy: 0.6812 - val_loss: 0.7880 - val_accuracy: 0.7280\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.9113 - accuracy: 0.6826 - val_loss: 0.8092 - val_accuracy: 0.7218\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.9002 - accuracy: 0.6886 - val_loss: 0.8307 - val_accuracy: 0.7115\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.8949 - accuracy: 0.6889 - val_loss: 0.7618 - val_accuracy: 0.7393\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.8878 - accuracy: 0.6918 - val_loss: 0.8279 - val_accuracy: 0.7133\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8811 - accuracy: 0.6972 - val_loss: 0.7584 - val_accuracy: 0.7394\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8754 - accuracy: 0.6973 - val_loss: 0.7751 - val_accuracy: 0.7348\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8704 - accuracy: 0.7019 - val_loss: 0.7880 - val_accuracy: 0.7285\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.8631 - accuracy: 0.7018 - val_loss: 0.7491 - val_accuracy: 0.7412\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8628 - accuracy: 0.7054 - val_loss: 0.7932 - val_accuracy: 0.7305\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8543 - accuracy: 0.7067 - val_loss: 0.7798 - val_accuracy: 0.7330\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8528 - accuracy: 0.7070 - val_loss: 0.7664 - val_accuracy: 0.7382\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.8499 - accuracy: 0.7082 - val_loss: 0.7805 - val_accuracy: 0.7357\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8465 - accuracy: 0.7086 - val_loss: 0.8641 - val_accuracy: 0.7157\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.8413 - accuracy: 0.7105 - val_loss: 0.7751 - val_accuracy: 0.7376\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8331 - accuracy: 0.7125 - val_loss: 0.7571 - val_accuracy: 0.7417\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.8342 - accuracy: 0.7140 - val_loss: 0.7302 - val_accuracy: 0.7552\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.8283 - accuracy: 0.7139 - val_loss: 0.7693 - val_accuracy: 0.7417\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.8278 - accuracy: 0.7165 - val_loss: 0.7779 - val_accuracy: 0.7382\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.8204 - accuracy: 0.7173 - val_loss: 0.7449 - val_accuracy: 0.7489\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.8208 - accuracy: 0.7194 - val_loss: 0.7153 - val_accuracy: 0.7546\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.8207 - accuracy: 0.7206 - val_loss: 0.7672 - val_accuracy: 0.7343\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.8147 - accuracy: 0.7215 - val_loss: 0.7479 - val_accuracy: 0.7443\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.8063 - accuracy: 0.7249 - val_loss: 0.7084 - val_accuracy: 0.7593\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.8083 - accuracy: 0.7224 - val_loss: 0.7442 - val_accuracy: 0.7484\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.8064 - accuracy: 0.7234 - val_loss: 0.7123 - val_accuracy: 0.7565\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.8037 - accuracy: 0.7275 - val_loss: 0.7890 - val_accuracy: 0.7318\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.7945 - accuracy: 0.7307 - val_loss: 0.7060 - val_accuracy: 0.7528\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.8003 - accuracy: 0.7275 - val_loss: 0.7011 - val_accuracy: 0.7612\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.7943 - accuracy: 0.7293 - val_loss: 0.6914 - val_accuracy: 0.7637\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.7984 - accuracy: 0.7259 - val_loss: 0.7511 - val_accuracy: 0.7450\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.7914 - accuracy: 0.7299 - val_loss: 0.6943 - val_accuracy: 0.7650\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7885 - accuracy: 0.7310 - val_loss: 0.7384 - val_accuracy: 0.7462\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7889 - accuracy: 0.7306 - val_loss: 0.7029 - val_accuracy: 0.7618\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.7878 - accuracy: 0.7321 - val_loss: 0.7071 - val_accuracy: 0.7559\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.7863 - accuracy: 0.7324 - val_loss: 0.6900 - val_accuracy: 0.7682\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7813 - accuracy: 0.7339 - val_loss: 0.7456 - val_accuracy: 0.7519\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.7802 - accuracy: 0.7361 - val_loss: 0.7183 - val_accuracy: 0.7606\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7783 - accuracy: 0.7359 - val_loss: 0.7404 - val_accuracy: 0.7556\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.7799 - accuracy: 0.7345 - val_loss: 0.7023 - val_accuracy: 0.7630\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.7743 - accuracy: 0.7383 - val_loss: 0.8164 - val_accuracy: 0.7232\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7719 - accuracy: 0.7393 - val_loss: 0.7096 - val_accuracy: 0.7586\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7772 - accuracy: 0.7364 - val_loss: 0.7106 - val_accuracy: 0.7590\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 0.7714 - accuracy: 0.7387 - val_loss: 0.7400 - val_accuracy: 0.7541\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 0.7690 - accuracy: 0.7397 - val_loss: 0.7147 - val_accuracy: 0.7527\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7735 - accuracy: 0.7386 - val_loss: 0.7114 - val_accuracy: 0.7688\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7690 - accuracy: 0.7387 - val_loss: 0.6588 - val_accuracy: 0.7806\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7681 - accuracy: 0.7405 - val_loss: 0.6811 - val_accuracy: 0.7708\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7636 - accuracy: 0.7425 - val_loss: 0.7218 - val_accuracy: 0.7611\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.7623 - accuracy: 0.7433 - val_loss: 0.7319 - val_accuracy: 0.7554\n",
            "Epoch 75/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.7615 - accuracy: 0.7433 - val_loss: 0.6595 - val_accuracy: 0.7802\n",
            "Epoch 76/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7637 - accuracy: 0.7436 - val_loss: 0.7613 - val_accuracy: 0.7417\n",
            "Epoch 77/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7644 - accuracy: 0.7403 - val_loss: 0.6885 - val_accuracy: 0.7687\n",
            "Epoch 78/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.7640 - accuracy: 0.7420 - val_loss: 0.6933 - val_accuracy: 0.7642\n",
            "Epoch 79/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.7611 - accuracy: 0.7440 - val_loss: 0.6907 - val_accuracy: 0.7621\n",
            "Epoch 80/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.7621 - accuracy: 0.7446 - val_loss: 0.7016 - val_accuracy: 0.7689\n",
            "Epoch 81/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7558 - accuracy: 0.7459 - val_loss: 0.7617 - val_accuracy: 0.7506\n",
            "Epoch 82/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7601 - accuracy: 0.7435 - val_loss: 0.7205 - val_accuracy: 0.7555\n",
            "Epoch 83/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7585 - accuracy: 0.7433 - val_loss: 0.6831 - val_accuracy: 0.7734\n",
            "Epoch 84/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7606 - accuracy: 0.7427 - val_loss: 0.7684 - val_accuracy: 0.7437\n",
            "Epoch 85/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7534 - accuracy: 0.7442 - val_loss: 0.6947 - val_accuracy: 0.7717\n",
            "Epoch 86/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7596 - accuracy: 0.7446 - val_loss: 0.6621 - val_accuracy: 0.7823\n",
            "Epoch 87/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7622 - accuracy: 0.7438 - val_loss: 0.7465 - val_accuracy: 0.7481\n",
            "Epoch 88/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7597 - accuracy: 0.7443 - val_loss: 0.6456 - val_accuracy: 0.7817\n",
            "Epoch 89/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7625 - accuracy: 0.7462 - val_loss: 0.7051 - val_accuracy: 0.7632\n",
            "Epoch 90/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7544 - accuracy: 0.7461 - val_loss: 0.7156 - val_accuracy: 0.7696\n",
            "Epoch 91/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7584 - accuracy: 0.7438 - val_loss: 0.7026 - val_accuracy: 0.7631\n",
            "Epoch 92/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7573 - accuracy: 0.7463 - val_loss: 0.6346 - val_accuracy: 0.7877\n",
            "Epoch 93/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7553 - accuracy: 0.7453 - val_loss: 0.6886 - val_accuracy: 0.7739\n",
            "Epoch 94/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7517 - accuracy: 0.7460 - val_loss: 0.7718 - val_accuracy: 0.7438\n",
            "Epoch 95/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7563 - accuracy: 0.7445 - val_loss: 0.7744 - val_accuracy: 0.7383\n",
            "Epoch 96/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7541 - accuracy: 0.7460 - val_loss: 0.7033 - val_accuracy: 0.7645\n",
            "Epoch 97/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7518 - accuracy: 0.7472 - val_loss: 0.6760 - val_accuracy: 0.7705\n",
            "Epoch 98/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7561 - accuracy: 0.7477 - val_loss: 0.6958 - val_accuracy: 0.7650\n",
            "Epoch 99/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7518 - accuracy: 0.7483 - val_loss: 0.6837 - val_accuracy: 0.7853\n",
            "Epoch 100/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7501 - accuracy: 0.7487 - val_loss: 0.6989 - val_accuracy: 0.7671\n",
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.7599 - accuracy: 0.3676 - val_loss: 1.5471 - val_accuracy: 0.4544\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.4952 - accuracy: 0.4618 - val_loss: 1.3559 - val_accuracy: 0.5087\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.3879 - accuracy: 0.5061 - val_loss: 1.3401 - val_accuracy: 0.5272\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.3045 - accuracy: 0.5346 - val_loss: 1.1749 - val_accuracy: 0.5890\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.2407 - accuracy: 0.5616 - val_loss: 1.1279 - val_accuracy: 0.6087\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.1833 - accuracy: 0.5831 - val_loss: 1.0972 - val_accuracy: 0.6143\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.1318 - accuracy: 0.6018 - val_loss: 1.0515 - val_accuracy: 0.6325\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0966 - accuracy: 0.6151 - val_loss: 0.9984 - val_accuracy: 0.6515\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0604 - accuracy: 0.6280 - val_loss: 0.9921 - val_accuracy: 0.6593\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0299 - accuracy: 0.6404 - val_loss: 0.9904 - val_accuracy: 0.6557\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9959 - accuracy: 0.6531 - val_loss: 0.9600 - val_accuracy: 0.6673\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9680 - accuracy: 0.6617 - val_loss: 0.8950 - val_accuracy: 0.6893\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9480 - accuracy: 0.6694 - val_loss: 0.8904 - val_accuracy: 0.6894\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.9211 - accuracy: 0.6819 - val_loss: 0.9232 - val_accuracy: 0.6804\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8998 - accuracy: 0.6867 - val_loss: 0.8934 - val_accuracy: 0.6956\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8788 - accuracy: 0.6949 - val_loss: 0.7919 - val_accuracy: 0.7314\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 0.8567 - accuracy: 0.7021 - val_loss: 0.8492 - val_accuracy: 0.7105\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8343 - accuracy: 0.7105 - val_loss: 0.8857 - val_accuracy: 0.6972\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8220 - accuracy: 0.7148 - val_loss: 0.8040 - val_accuracy: 0.7242\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.8029 - accuracy: 0.7250 - val_loss: 0.7830 - val_accuracy: 0.7350\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 31s 20ms/step - loss: 0.7911 - accuracy: 0.7274 - val_loss: 0.7801 - val_accuracy: 0.7328\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7676 - accuracy: 0.7344 - val_loss: 0.7266 - val_accuracy: 0.7552\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7589 - accuracy: 0.7362 - val_loss: 0.7853 - val_accuracy: 0.7317\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 30s 19ms/step - loss: 0.7464 - accuracy: 0.7434 - val_loss: 0.7653 - val_accuracy: 0.7425\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 29s 19ms/step - loss: 0.7306 - accuracy: 0.7458 - val_loss: 0.7378 - val_accuracy: 0.7541\n",
            "Epoch 26/100\n",
            "  39/1563 [..............................] - ETA: 26s - loss: 0.7342 - accuracy: 0.7364"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW4--P5O2jzV"
      },
      "source": [
        "From the two graphs, we can see the one without dropout and without augmentation learns the fastest but overfits the most. Data augmentation helps tremendously to improve generalization. Dropout\n",
        "helps to improve generalization when there is no data augmentation, but it worsens the results when there is data\n",
        "augmentation."
      ]
    }
  ]
}